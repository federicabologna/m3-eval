{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Experiments - All Perturbations\n",
    "\n",
    "This notebook runs baseline rating experiments for all perturbations using Qwen3-8B.\n",
    "\n",
    "**Perturbations:**\n",
    "- `add_typos` (0.3, 0.5, 0.7 probability)\n",
    "- `change_dosage`\n",
    "- `remove_sentences` (30%, 50%, 70%)\n",
    "- `add_confusion`\n",
    "\n",
    "**Levels:**\n",
    "- `coarse` (answer-level)\n",
    "- `fine` (sentence-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Add code directory to path\n",
    "project_root = Path.cwd().parent\n",
    "code_dir = project_root / 'code'\n",
    "sys.path.insert(0, str(code_dir))\n",
    "\n",
    "from helpers.experiment_utils import (\n",
    "    setup_paths,\n",
    "    load_qa_data,\n",
    "    get_processed_ids,\n",
    "    clean_model_name,\n",
    "    get_id_key,\n",
    "    get_or_create_perturbations,\n",
    "    get_or_create_original_ratings,\n",
    "    save_result\n",
    ")\n",
    "from helpers.multi_llm_inference import get_provider_from_model\n",
    "from perturbation_pipeline import load_prompt, get_rating_with_averaging\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Experiment configuration\nMODEL = 'Qwen3-8B'\nSEED = 42\nNUM_RUNS = 5  # Number of rating runs to average\n\n# Which perturbations to run (set to None to run all)\nPERTURBATIONS = None  # ['add_typos', 'change_dosage', 'remove_sentences', 'add_confusion']\n\n# Which level to run\nLEVEL = 'both'  # 'coarse', 'fine', or 'both'\n\n# BACKUP CONFIGURATION\n# Set this to the directory where you want to backup existing results before running new experiments\n# If None, no backup will be created\nBACKUP_DIR = project_root / 'output' / 'cqa_eval' / 'experiment_results' / 'backup'\n# Example: BACKUP_DIR = project_root / 'output' / 'cqa_eval' / 'experiment_results' / 'backup_2026_01_30'\n\n# Set random seed\nrandom.seed(SEED)\n\nprint(f\"Model: {MODEL}\")\nprint(f\"Provider: {get_provider_from_model(MODEL)}\")\nprint(f\"Random seed: {SEED}\")\nprint(f\"Level: {LEVEL}\")\nif BACKUP_DIR:\n    print(f\"Backup directory: {BACKUP_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Paths and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "paths = setup_paths()\n",
    "output_dir = paths['output_dir']\n",
    "model_name_clean = clean_model_name(MODEL)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Coarse data: {paths['coarse_data_path']}\")\n",
    "print(f\"Fine data: {paths['fine_data_path']}\")\n",
    "print(f\"Prompts: {paths['prompts_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All perturbations with their parameter variations\n",
    "all_perturbations = {\n",
    "    'add_typos': {'typo_prob': [0.3, 0.5, 0.7], 'remove_pct': [0.3]},\n",
    "    'change_dosage': {'typo_prob': [0.5], 'remove_pct': [0.3]},\n",
    "    'remove_sentences': {'typo_prob': [0.5], 'remove_pct': [0.3, 0.5, 0.7]},\n",
    "    'add_confusion': {'typo_prob': [0.5], 'remove_pct': [0.3]}\n",
    "}\n",
    "\n",
    "if PERTURBATIONS is None:\n",
    "    perturbations_to_run = all_perturbations\n",
    "else:\n",
    "    perturbations_to_run = {k: all_perturbations[k] for k in PERTURBATIONS}\n",
    "\n",
    "print(f\"Perturbations to run: {list(perturbations_to_run.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Backup Existing Results\n\nBefore running experiments, backup any existing result files to preserve previous runs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import shutil\nfrom datetime import datetime\n\ndef backup_existing_results(backup_dir, output_dir, model_name_clean, perturbations_to_run, levels):\n    \"\"\"\n    Backup existing result files before running new experiments.\n    \n    Args:\n        backup_dir: Path to backup directory\n        output_dir: Base output directory\n        model_name_clean: Cleaned model name\n        perturbations_to_run: Dict of perturbations to process\n        levels: List of levels to process\n    \n    Returns:\n        Number of files backed up\n    \"\"\"\n    if backup_dir is None:\n        print(\"‚ö†Ô∏è  No backup directory configured. Skipping backup.\")\n        return 0\n    \n    # Create backup directory with timestamp\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    backup_path = Path(backup_dir) / f\"backup_{timestamp}\"\n    backup_path.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"üì¶ Backing up existing results to: {backup_path}\")\n    \n    baseline_dir = Path(output_dir) / 'experiment_results' / 'baseline'\n    files_backed_up = 0\n    \n    for level in levels:\n        for perturbation_name, params in perturbations_to_run.items():\n            perturbation_dir = baseline_dir / perturbation_name\n            \n            if not perturbation_dir.exists():\n                continue\n            \n            # Iterate over parameter combinations to find all possible files\n            for remove_pct in params['remove_pct']:\n                for typo_prob in params['typo_prob']:\n                    # Determine output filename\n                    if perturbation_name == 'remove_sentences':\n                        pct_str = str(int(remove_pct * 100))\n                        output_filename = f\"{perturbation_name}_{pct_str}pct_{level}_{model_name_clean}_rating.jsonl\"\n                    elif perturbation_name == 'add_typos':\n                        prob_str = str(typo_prob).replace('.', '')\n                        output_filename = f\"{perturbation_name}_{prob_str}prob_{level}_{model_name_clean}_rating.jsonl\"\n                    else:\n                        output_filename = f\"{perturbation_name}_{level}_{model_name_clean}_rating.jsonl\"\n                    \n                    source_file = perturbation_dir / output_filename\n                    \n                    if source_file.exists():\n                        # Create perturbation subdirectory in backup\n                        backup_subdir = backup_path / perturbation_name\n                        backup_subdir.mkdir(parents=True, exist_ok=True)\n                        \n                        # Copy file to backup\n                        dest_file = backup_subdir / output_filename\n                        shutil.copy2(source_file, dest_file)\n                        files_backed_up += 1\n                        print(f\"  ‚úì Backed up: {perturbation_name}/{output_filename}\")\n    \n    if files_backed_up > 0:\n        print(f\"\\n‚úÖ Backed up {files_backed_up} files to: {backup_path}\")\n    else:\n        print(f\"\\n‚ÑπÔ∏è  No existing files to backup\")\n    \n    return files_backed_up\n\n# Perform backup\nlevels = ['coarse', 'fine'] if LEVEL == 'both' else [LEVEL]\nbackup_existing_results(BACKUP_DIR, output_dir, model_name_clean, perturbations_to_run, levels)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Determine which levels to process\nlevels = ['coarse', 'fine'] if LEVEL == 'both' else [LEVEL]\n\nfor level in levels:\n    print(f\"\\n{'='*80}\")\n    print(f\"LEVEL: {level.upper()}\")\n    print(f\"{'='*80}\")\n\n    # Load data\n    data_path = paths['coarse_data_path'] if level == 'coarse' else paths['fine_data_path']\n    all_qa_pairs = load_qa_data(data_path)\n    print(f\"Loaded {len(all_qa_pairs)} QA pairs\")\n\n    # For fine level, filter to only IDs that exist in original ratings\n    if level == 'fine':\n        original_ratings_filename = f\"original_{level}_{model_name_clean}_rating.jsonl\"\n        original_ratings_path = os.path.join(output_dir, 'original_ratings', original_ratings_filename)\n\n        if os.path.exists(original_ratings_path):\n            # Load IDs from original ratings file\n            rated_ids = set()\n            with open(original_ratings_path, 'r') as f:\n                for line in f:\n                    entry = json.loads(line)\n                    # Get ID using the appropriate key\n                    id_key_temp = get_id_key([entry])\n                    rated_ids.add(entry[id_key_temp])\n\n            print(f\"Found {len(rated_ids)} entries in original fine ratings file\")\n            print(f\"Filtering fine data to only process these {len(rated_ids)} entries\")\n\n            # Filter qa_pairs to only include rated IDs\n            id_key = get_id_key(all_qa_pairs)\n            qa_pairs = [qa for qa in all_qa_pairs if qa[id_key] in rated_ids]\n            print(f\"After filtering: {len(qa_pairs)} examples to process\")\n        else:\n            print(f\"Warning: Original ratings file not found at {original_ratings_path}\")\n            print(f\"Processing all {len(all_qa_pairs)} fine examples\")\n            qa_pairs = all_qa_pairs\n    else:\n        qa_pairs = all_qa_pairs\n\n    id_key = get_id_key(qa_pairs)\n\n    # Select prompt\n    prompt_path = os.path.join(paths['prompts_dir'], f'{level}prompt_system.txt')\n\n    # Step 1: Get/compute original ratings\n    print(f\"\\n{'-'*80}\")\n    print(\"STEP 1: ORIGINAL RATINGS\")\n    print(f\"{'-'*80}\")\n\n    # For fine level, skip computing missing ratings (use only existing ones)\n    skip_missing = (level == 'fine')\n\n    original_ratings_dict = get_or_create_original_ratings(\n        qa_pairs=qa_pairs,\n        level=level,\n        prompt_path=prompt_path,\n        model=MODEL,\n        output_dir=output_dir,\n        model_name_clean=model_name_clean,\n        num_runs=NUM_RUNS,\n        skip_missing=skip_missing\n    )\n\n    # Filter qa_pairs to only include IDs that have original ratings\n    # This is especially important for fine level where we skip computing missing ratings\n    qa_pairs = [qa for qa in qa_pairs if qa[id_key] in original_ratings_dict]\n    print(f\"‚úì Processing {len(qa_pairs)} examples with original ratings\")\n\n    # Step 2: Process each perturbation\n    print(f\"\\n{'-'*80}\")\n    print(\"STEP 2: PERTURBATIONS\")\n    print(f\"{'-'*80}\")\n\n    # Create baseline experiment directory\n    baseline_dir = os.path.join(output_dir, 'experiment_results', 'baseline')\n    os.makedirs(baseline_dir, exist_ok=True)\n\n    for perturbation_name, params in perturbations_to_run.items():\n        print(f\"\\n[{perturbation_name.upper()}]\")\n\n        # Create perturbation-specific subdirectory\n        perturbation_dir = os.path.join(baseline_dir, perturbation_name)\n        os.makedirs(perturbation_dir, exist_ok=True)\n\n        # Iterate over parameter combinations\n        for remove_pct in params['remove_pct']:\n            for typo_prob in params['typo_prob']:\n                # Determine output filename\n                if perturbation_name == 'remove_sentences':\n                    pct_str = str(int(remove_pct * 100))\n                    output_filename = f\"{perturbation_name}_{pct_str}pct_{level}_{model_name_clean}_rating.jsonl\"\n                elif perturbation_name == 'add_typos':\n                    prob_str = str(typo_prob).replace('.', '')\n                    output_filename = f\"{perturbation_name}_{prob_str}prob_{level}_{model_name_clean}_rating.jsonl\"\n                else:\n                    output_filename = f\"{perturbation_name}_{level}_{model_name_clean}_rating.jsonl\"\n\n                output_path = os.path.join(perturbation_dir, output_filename)\n\n                # Check which entries have already been processed\n                processed_ids = get_processed_ids(output_path)\n                remaining_qa_pairs = [qa for qa in qa_pairs if qa[id_key] not in processed_ids]\n\n                if len(remaining_qa_pairs) == 0:\n                    print(f\"  ‚úì {output_filename}: All {len(qa_pairs)} entries complete\")\n                    continue\n\n                print(f\"  Processing: {output_filename}\")\n                print(f\"    {len(remaining_qa_pairs)} remaining (out of {len(qa_pairs)})\")\n\n                # Load or generate perturbations\n                perturbations_dict = get_or_create_perturbations(\n                    perturbation_name=perturbation_name,\n                    level=level,\n                    qa_pairs=qa_pairs,\n                    typo_prob=typo_prob,\n                    remove_pct=remove_pct,\n                    seed=SEED,\n                    output_dir=output_dir\n                )\n\n                # Process each QA pair\n                import time\n                for idx, qa_pair in enumerate(remaining_qa_pairs, 1):\n                    question = qa_pair['question']\n                    original_answer = qa_pair['answer']\n\n                    # Get pre-generated perturbation\n                    perturbation_entry = perturbations_dict.get(qa_pair[id_key])\n\n                    if perturbation_entry is None:\n                        print(f\"    Skipping {qa_pair[id_key]} - no perturbation found\")\n                        continue\n\n                    perturbed_answer = perturbation_entry['perturbed_answer']\n\n                    # Get perturbed rating\n                    print(f\"    [{idx}/{len(remaining_qa_pairs)}] {qa_pair[id_key]}...\", end=\" \")\n                    start_time = time.time()\n                    perturbed_rating = get_rating_with_averaging(\n                        question, perturbed_answer, *load_prompt(prompt_path),\n                        MODEL, num_runs=NUM_RUNS\n                    )\n                    elapsed_time = time.time() - start_time\n                    print(f\"{elapsed_time:.1f}s\")\n\n                    # Get original rating from dict\n                    original_rating = original_ratings_dict.get(qa_pair[id_key])\n\n                    if original_rating is None:\n                        print(f\"    WARNING: No original rating found for {qa_pair[id_key]}, skipping...\")\n                        continue\n\n                    # Build result\n                    result = qa_pair.copy()\n                    result['perturbation'] = perturbation_name\n                    result['perturbed_answer'] = perturbed_answer\n                    result['original_rating'] = original_rating\n                    result['perturbed_rating'] = perturbed_rating\n                    result['random_seed'] = SEED\n\n                    # Add perturbation metadata\n                    for key in ['typo_probability', 'removal_percentage', 'change_counts', 'skip_reason']:\n                        if key in perturbation_entry:\n                            result[key] = perturbation_entry[key]\n\n                    # Save to file\n                    save_result(output_path, result)\n\nprint(f\"\\n{'='*80}\")\nprint(\"BASELINE EXPERIMENTS COMPLETED\")\nprint(f\"{'='*80}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Results saved to: `output/cqa_eval/experiment_results/baseline/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}